================================================================================
ЛАБОРАТОРНА РОБОТА 6. ЧАСТИНА 2. ІНСТРУМЕНТИ
================================================================================

Завдання:
Створити CLI інструмент, який дозволяє застосувати DML/DDL скрипти на всіх 16 БД. 
Інструмент повинен реалізовувати механізм розподіленої транзакції: Commit робиться 
лише якщо на всіх 16 БД сценарій виконався вдало.

================================================================================
1. АРХІТЕКТУРА РІШЕННЯ
================================================================================

Для реалізації завдання розроблено скрипт `tools/shard_executor.py`. 
Він використовує патерн Two-Phase Commit (2PC) на рівні додатку для забезпечення 
атомарності операцій між 16 незалежними базами даних.

Алгоритм роботи:

1. Ініціалізація:
   - Завантаження конфігурації з `mapping.json`.
   - Встановлення з'єднання з усіма 16 шардами (db_0..db_f).

2. Фаза 1 (Execute/Prepare):
   - Відкриття транзакцій (`BEGIN`) на кожному шарді.
   - Виконання SQL-скрипта (`session.execute()`) на кожному шарді.
   - `session.flush()` для відправки команд в БД без фіксації.
   - Якщо на будь-якому шарді виникає помилка -> перехід до Rollback.

3. Фаза 2 (Commit/Rollback):
   - Якщо Фаза 1 пройшла успішно на ВСІХ шардах:
     - Виконується `session.commit()` послідовно для кожного шарду.
   - Якщо виникла помилка:
     - Виконується `session.rollback()` для всіх відкритих сесій.

Цей підхід гарантує, що DDL/DML зміни будуть застосовані або всюди, або ніде 
(з точністю до збоїв під час самої фази коміту, що є відомим обмеженням 2PC).

================================================================================
2. РЕАЛІЗАЦІЯ (tools/shard_executor.py)
================================================================================

Ключовий фрагмент коду, що відповідає за розподілену транзакцію:

```python
    try:
        # 1. Відкриваємо сесії для всіх шардів
        for shard_name in shards:
            session = manager.get_db_session(shard_name)
            sessions[shard_name] = session

        # 2. Фаза виконання (без коміту)
        for shard_name, session in sessions.items():
            logger.info(f"Executing SQL on {shard_name}...")
            session.execute(text(sql_content))
            session.flush()
        
        # 3. Фаза коміту (тільки якщо все ок)
        for shard_name, session in sessions.items():
            session.commit()

    except Exception as e:
        logger.error(f"Error: {e}. Rolling back all transactions...")
        # 4. Rollback всюди
        for session in sessions.values():
            session.rollback()
        raise
```

================================================================================
3. ІНСТРУКЦІЯ З ВИКОРИСТАННЯ
================================================================================

Оскільки файл `mapping.json` містить імена хостів Docker мережі (`postgres_00` тощо), 
скрипт необхідно запускати **всередині контейнера додатку**.

1. Скопіюйте інструменти в контейнер (якщо вони не примонтовані через volume):
   *(Якщо ви використовуєте volume ./:/app, цей крок не потрібен)*
   
   ```bash
   # Якщо volume не налаштовано
   docker compose -f docker-compose-sharding.yml cp tools app:/app/
   ```

2. Запустіть скрипт всередині контейнера:

   ```bash
   docker compose -f docker-compose-sharding.yml exec app python tools/shard_executor.py tools/test_migration.sql
   ```

Очікуваний вивід (успішний):
```
INFO - Starting distributed execution...
INFO - Connecting to db_0...
...
INFO - Executing SQL on db_0...
...
INFO - SQL executed successfully on all shards. Committing...
INFO - Committed on db_0
...
INFO - Distributed execution completed successfully.
```

================================================================================
4. ВИСНОВКИ
================================================================================

Розроблений CLI інструмент успішно вирішує задачу синхронного оновлення схеми та даних
у всіх шардах кластеру. Реалізація механізму Two-Phase Commit на рівні додатку забезпечує
узгодженість даних, гарантуючи, що міграції будуть застосовані атомарно до всього кластеру або відхилені повністю у разі збою.

